1. Lokální spuštění (bez Dockeru)
Tato možnost je vhodná pro vývoj a testování. Aplikace bude fungovat, ale nebudete mít k dispozici Ollama pro AI zpracování faktur.

python3 run.py --reload

2. Spuštění pomocí Dockeru
Tato možnost je vhodná pro produkční prostředí. Aplikace bude fungovat s Ollama pro AI zpracování faktur.

docker compose down
docker compose up --build

Sestaví Docker image pro vaši aplikaci podle Dockerfile
Spustí kontejner s Ollama serverem
Spustí kontejner s vaší aplikací
Propojí je, aby spolu mohly komunikovat
Aplikace bude dostupná na http://localhost:8000 a Ollama API na http://localhost:11434.

První spuštění s Dockerem

docker compose ps

Při prvním spuštění bude Docker stahovat potřebné image a instalovat model pro Ollama, což může chvíli trvat. Doporučuji sledovat logy:

docker compose logs -f

Stažení modelu pro Ollama
Po prvním spuštění je potřeba stáhnout model pro Ollama (pokud se nestáhne automaticky):

docker compose exec ollama ollama pull llama3
nebo
docker exec -it ollama ollama pull llama3

nebo stažení modelu mistral:

docker compose exec ollama ollama pull mistral
nebo
docker exec -it ollama ollama pull mistral


Pokud jsme aplikaci nespouštěli takji můžeme spustitpomocí:

python3 run.py --reload

Další kroky při změně kódu:
Restartujte kontejnery, aby se změny projevily:
docker compose down
docker compose up --build

Sledujte logy, zda se nevyskytují další chyby:
docker compose logs -f